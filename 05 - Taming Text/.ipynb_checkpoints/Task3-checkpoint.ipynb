{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"hillary-clinton-emails/Emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Id', 'DocNumber', 'MetadataSubject', 'MetadataTo', 'MetadataFrom',\n",
       "       'SenderPersonId', 'MetadataDateSent', 'MetadataDateReleased',\n",
       "       'MetadataPdfLink', 'MetadataCaseNumber', 'MetadataDocumentClass',\n",
       "       'ExtractedSubject', 'ExtractedTo', 'ExtractedFrom', 'ExtractedCc',\n",
       "       'ExtractedDateSent', 'ExtractedCaseNumber', 'ExtractedDocNumber',\n",
       "       'ExtractedDateReleased', 'ExtractedReleaseInPartOrFull',\n",
       "       'ExtractedBodyText', 'RawText'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we aggregate over ExtractedSubject which represents the topics of happen conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsDocuments = emails[['ExtractedSubject','ExtractedBodyText']].dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remplace all non-alphabetical characters with a whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsDocuments.ExtractedSubject = emailsDocuments.ExtractedSubject.apply(lambda x: re.sub('[^a-zA-Z\\-]+', ' ', x))\n",
    "emailsDocuments.ExtractedBodyText = emailsDocuments.ExtractedBodyText.apply(lambda x: re.sub('[^a-zA-Z\\-]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedemails = emailsDocuments.groupby('ExtractedSubject')['ExtractedBodyText'].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedemails['Documents'] = groupedemails.ExtractedSubject.astype(str).str.cat(groupedemails.ExtractedBodyText.astype(str), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupedemails['Documents'] = groupedemails.RawText  ExtractedBodyText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing will be to implement the pipline to clean our texts and remove email oriented words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customstopwords = stopwords.words('english')\n",
    "customstopwords = customstopwords + ['call', 'know', 'would', 'get', 'time', 'work', 'like', 'today', \n",
    "                                     'see', 'morning', 'also', 'back', 'tomorrow', 'meeting', 'think', \n",
    "                                     'good', 'want', 'could', 'working', 'well', 'fw', 'fyi', 're', 'pm', 'h',\n",
    "                                     'subject',\n",
    "                                     'a','about','above','across','after','again','against','all','almost',\n",
    "                                     'alone','along','already','also','although','always','among','an','and',\n",
    "                                     'another','any','anybody','anyone','anything','anywhere','are','area',\n",
    "                                     'areas','around','as','ask','asked','asking','asks','at','away','b','back',\n",
    "                                     'backed','backing','backs','be','became','because','become','becomes','been',\n",
    "                                     'before','began','behind','being','beings','best','better','between','big',\n",
    "                                     'both','but','by','c','came','can','cannot','case','cases','certain',\n",
    "                                     'certainly','clear','clearly','come','could','d','did','differ','different',\n",
    "                                     'differently','do','does','done','down','down','downed','downing','downs',\n",
    "                                     'during','e','each','early','either','end','ended','ending','ends','enough',\n",
    "                                     'even','evenly','ever','every','everybody','everyone','everything',\n",
    "                                     'everywhere','f','face','faces','fact','facts','far','felt','few','find',\n",
    "                                     'finds','first','for','four','from','full','fully','further','furthered',\n",
    "                                     'furthering','furthers','g','gave','general','generally','get','gets','give',\n",
    "                                     'given','gives','go','going','good','goods','got','great','greater','greatest',\n",
    "                                     'group','grouped','grouping','groups','h','had','has','have','having','he','her',\n",
    "                                     'here','herself','high','high','high','higher','highest','him','himself','his',\n",
    "                                     'how','however','i','if','important','in','interest','interested','interesting',\n",
    "                                     'interests','into','is','it','its','itself','j','just','k','keep','keeps','kind',\n",
    "                                     'knew','know','known','knows','l','large','largely','last','later','latest',\n",
    "                                     'least','less','let','lets','like','likely','long','longer','longest','m',\n",
    "                                     'made','make','making','man','many','may','me','member','members','men',\n",
    "                                     'might','more','most','mostly','mr','mrs','much','must','my','myself',\n",
    "                                     'n','necessary','need','needed','needing','needs','never','new','new',\n",
    "                                     'newer','newest','next','no','nobody','non','noone','not','nothing',\n",
    "                                     'now','nowhere','number','numbers','o','of','off','often','old','older',\n",
    "                                     'oldest','on','once','one','only','open','opened','opening','opens','or',\n",
    "                                     'order','ordered','ordering','orders','other','others','our','out','over',\n",
    "                                     'p','part','parted','parting','parts','per','perhaps','place','places',\n",
    "                                     'point','pointed','pointing','points','possible','present','presented',\n",
    "                                     'presenting','presents','problem','problems','put','puts','q','quite','r',\n",
    "                                     'rather','really','right','right','room','rooms','s','said','same','saw',\n",
    "                                     'say','says','second','seconds','see','seem','seemed','seeming','seems',\n",
    "                                     'sees','several','shall','she','should','show','showed','showing','shows',\n",
    "                                     'side','sides','since','small','smaller','smallest','so','some','somebody',\n",
    "                                     'someone','something','somewhere','state','states','still','still','such',\n",
    "                                     'sure','t','take','taken','than','that','the','their','them','then','there',\n",
    "                                     'therefore','these','they','thing','things','think','thinks','this','those',\n",
    "                                     'though','thought','thoughts','three','through','thus','to','today','together',\n",
    "                                     'too','took','toward','turn','turned','turning','turns','two','u','under',\n",
    "                                     'until','up','upon','us','use','used','uses','v','very','w','want',\n",
    "                                     'wanted','wanting','wants','was','way','ways','we','well','wells','went','were',\n",
    "                                     'what','when','where','whether','which','while','who','whole','whose','why',\n",
    "                                     'will','with','within','without','work','worked','working','works','would','x',\n",
    "                                     'y','year','years','yet','you','young','younger','youngest','your','yours','z']\n",
    "def format(text):\n",
    "    words = [w.lower() for w in text.split() if len(w)>3]\n",
    "    wordswostop = [w for w in words if w not in customstopwords]\n",
    "    return wordswostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [format(doc) for doc in groupedemails.Documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"benghazi\" + 0.009*\"information\" + 0.009*\"house\" + 0.009*\"department\" + 0.008*\"date\" + 0.008*\"agreement\" + 0.008*\"comm\" + 0.008*\"dept\" + 0.008*\"waiver\" + 0.007*\"produced\"'),\n",
       " (1,\n",
       "  '0.009*\"china\" + 0.007*\"district\" + 0.005*\"iran\" + 0.004*\"secretary\" + 0.004*\"holbrooke\" + 0.004*\"women\" + 0.003*\"house\" + 0.003*\"ambassador\" + 0.003*\"world\" + 0.003*\"united\"'),\n",
       " (2,\n",
       "  '0.007*\"house\" + 0.004*\"heyman\" + 0.004*\"people\" + 0.003*\"policy\" + 0.003*\"book\" + 0.003*\"called\" + 0.003*\"richards\" + 0.003*\"bill\" + 0.003*\"support\" + 0.003*\"vote\"'),\n",
       " (3,\n",
       "  '0.007*\"president\" + 0.005*\"office\" + 0.004*\"obama\" + 0.004*\"speech\" + 0.004*\"discuss\" + 0.003*\"washington\" + 0.003*\"percent\" + 0.003*\"please\" + 0.003*\"public\" + 0.003*\"print\"'),\n",
       " (4,\n",
       "  '0.008*\"mail\" + 0.006*\"message\" + 0.006*\"received\" + 0.005*\"read\" + 0.005*\"clinton\" + 0.005*\"secretary\" + 0.004*\"start\" + 0.004*\"address\" + 0.004*\"local\" + 0.003*\"system\"'),\n",
       " (5,\n",
       "  '0.008*\"secretary\" + 0.005*\"people\" + 0.005*\"house\" + 0.005*\"president\" + 0.004*\"security\" + 0.004*\"department\" + 0.004*\"world\" + 0.004*\"office\" + 0.003*\"support\" + 0.003*\"clinton\"'),\n",
       " (6,\n",
       "  '0.011*\"obama\" + 0.006*\"clinton\" + 0.005*\"israel\" + 0.004*\"president\" + 0.004*\"american\" + 0.004*\"house\" + 0.004*\"people\" + 0.003*\"administration\" + 0.003*\"print\" + 0.003*\"political\"'),\n",
       " (7,\n",
       "  '0.007*\"report\" + 0.007*\"secretary\" + 0.006*\"women\" + 0.005*\"holbrooke\" + 0.004*\"afghan\" + 0.003*\"office\" + 0.003*\"thank\" + 0.003*\"gordon\" + 0.003*\"trip\" + 0.003*\"minister\"'),\n",
       " (8,\n",
       "  '0.007*\"office\" + 0.005*\"secretary\" + 0.005*\"images\" + 0.004*\"foreign\" + 0.004*\"minister\" + 0.004*\"approve\" + 0.004*\"popover\" + 0.004*\"amazon\" + 0.004*\"vote\" + 0.004*\"disapprove\"'),\n",
       " (9,\n",
       "  '0.007*\"iran\" + 0.005*\"talk\" + 0.004*\"nuclear\" + 0.004*\"week\" + 0.004*\"speech\" + 0.004*\"tonight\" + 0.004*\"final\" + 0.003*\"foreign\" + 0.003*\"cheryl\" + 0.003*\"draft\"'),\n",
       " (10,\n",
       "  '0.007*\"obama\" + 0.007*\"clinton\" + 0.006*\"president\" + 0.004*\"afghan\" + 0.004*\"people\" + 0.004*\"gore\" + 0.004*\"security\" + 0.003*\"foreign\" + 0.003*\"percent\" + 0.003*\"national\"'),\n",
       " (11,\n",
       "  '0.006*\"president\" + 0.006*\"palau\" + 0.005*\"obama\" + 0.005*\"united\" + 0.004*\"people\" + 0.004*\"foreign\" + 0.004*\"libya\" + 0.004*\"democrats\" + 0.004*\"democratic\" + 0.004*\"honduras\"'),\n",
       " (12,\n",
       "  '0.010*\"mcchrystal\" + 0.007*\"afghanistan\" + 0.005*\"office\" + 0.004*\"president\" + 0.004*\"secretary\" + 0.004*\"afghan\" + 0.004*\"obama\" + 0.004*\"government\" + 0.003*\"talk\" + 0.003*\"agreement\"'),\n",
       " (13,\n",
       "  '0.015*\"secretary\" + 0.013*\"office\" + 0.009*\"department\" + 0.006*\"haiti\" + 0.006*\"foreign\" + 0.005*\"route\" + 0.005*\"arrive\" + 0.005*\"depart\" + 0.005*\"residence\" + 0.005*\"american\"'),\n",
       " (14,\n",
       "  '0.013*\"office\" + 0.011*\"secretary\" + 0.010*\"obama\" + 0.007*\"department\" + 0.005*\"president\" + 0.004*\"private\" + 0.004*\"conference\" + 0.004*\"house\" + 0.004*\"depart\" + 0.004*\"arrive\"'),\n",
       " (15,\n",
       "  '0.005*\"women\" + 0.004*\"department\" + 0.003*\"secretary\" + 0.003*\"schedule\" + 0.003*\"monday\" + 0.003*\"policy\" + 0.003*\"house\" + 0.003*\"bloomberg\" + 0.003*\"president\" + 0.003*\"issues\"'),\n",
       " (16,\n",
       "  '0.009*\"obama\" + 0.005*\"president\" + 0.005*\"israeli\" + 0.005*\"house\" + 0.004*\"press\" + 0.004*\"white\" + 0.004*\"mcchrystal\" + 0.004*\"clinton\" + 0.004*\"percent\" + 0.004*\"health\"'),\n",
       " (17,\n",
       "  '0.006*\"obama\" + 0.005*\"party\" + 0.005*\"president\" + 0.004*\"beck\" + 0.003*\"political\" + 0.003*\"republican\" + 0.003*\"calls\" + 0.003*\"house\" + 0.003*\"skousen\" + 0.003*\"american\"'),\n",
       " (18,\n",
       "  '0.008*\"president\" + 0.006*\"clinton\" + 0.006*\"branch\" + 0.005*\"blair\" + 0.005*\"deal\" + 0.005*\"china\" + 0.004*\"political\" + 0.004*\"agreement\" + 0.003*\"iran\" + 0.003*\"world\"'),\n",
       " (19,\n",
       "  '0.009*\"secretary\" + 0.005*\"office\" + 0.005*\"email\" + 0.004*\"lona\" + 0.004*\"assistant\" + 0.004*\"ireland\" + 0.004*\"special\" + 0.004*\"speech\" + 0.004*\"northern\" + 0.004*\"week\"')]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 20\n",
    "lda = LdaMulticore(corpus, num_topics=N, workers=3, id2word=dictionary, alpha=0.02, eta=0.02, \n",
    "                   iterations=1000, random_state=42)\n",
    "lda.print_topics(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
