{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hal/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"hillary-clinton-emails/Emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Id', 'DocNumber', 'MetadataSubject', 'MetadataTo', 'MetadataFrom',\n",
       "       'SenderPersonId', 'MetadataDateSent', 'MetadataDateReleased',\n",
       "       'MetadataPdfLink', 'MetadataCaseNumber', 'MetadataDocumentClass',\n",
       "       'ExtractedSubject', 'ExtractedTo', 'ExtractedFrom', 'ExtractedCc',\n",
       "       'ExtractedDateSent', 'ExtractedCaseNumber', 'ExtractedDocNumber',\n",
       "       'ExtractedDateReleased', 'ExtractedReleaseInPartOrFull',\n",
       "       'ExtractedBodyText', 'RawText'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we aggregate over ExtractedSubject which represents the topics of happen conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsDocuments = emails[['ExtractedSubject','ExtractedBodyText']].fillna('nan').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remplace all non-alphabetical characters with a whitespace. Group conversations by subject. After we join all texts in one group in one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emailsDocuments.ExtractedSubject = emailsDocuments.ExtractedSubject.apply(lambda x: re.sub('[^a-zA-Z\\-]+', ' ', x))\n",
    "emailsDocuments.ExtractedBodyText = emailsDocuments.ExtractedBodyText.apply(lambda x: re.sub('[^a-zA-Z\\-]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedemails = emailsDocuments.groupby('ExtractedSubject')['ExtractedBodyText'].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedemails['Documents'] = groupedemails.ExtractedSubject.astype(str).str.cat(groupedemails.ExtractedBodyText.astype(str), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupedemails['Documents'] = groupedemails.RawText  ExtractedBodyText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing will be to implement the pipline to clean our texts and remove email oriented words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customstopwords = stopwords.words('english')\n",
    "customstopwords = customstopwords + ['nan','call', 'know', 'would', 'get', 'time', 'work', 'like', 'today', \n",
    "                                     'see', 'morning', 'also', 'back', 'tomorrow', 'meeting', 'think', \n",
    "                                     'good', 'want', 'could', 'working', 'well', 'fw', 'fyi', 're', 'pm', 'h',\n",
    "                                     'subject',\n",
    "                                     'a','about','above','across','after','again','against','all','almost',\n",
    "                                     'alone','along','already','also','although','always','among','an','and',\n",
    "                                     'another','any','anybody','anyone','anything','anywhere','are','area',\n",
    "                                     'areas','around','as','ask','asked','asking','asks','at','away','b','back',\n",
    "                                     'backed','backing','backs','be','became','because','become','becomes','been',\n",
    "                                     'before','began','behind','being','beings','best','better','between','big',\n",
    "                                     'both','but','by','c','came','can','cannot','case','cases','certain',\n",
    "                                     'certainly','clear','clearly','come','could','d','did','differ','different',\n",
    "                                     'differently','do','does','done','down','down','downed','downing','downs',\n",
    "                                     'during','e','each','early','either','end','ended','ending','ends','enough',\n",
    "                                     'even','evenly','ever','every','everybody','everyone','everything',\n",
    "                                     'everywhere','f','face','faces','fact','facts','far','felt','few','find',\n",
    "                                     'finds','first','for','four','from','full','fully','further','furthered',\n",
    "                                     'furthering','furthers','g','gave','general','generally','get','gets','give',\n",
    "                                     'given','gives','go','going','good','goods','got','great','greater','greatest',\n",
    "                                     'group','grouped','grouping','groups','h','had','has','have','having','he','her',\n",
    "                                     'here','herself','high','high','high','higher','highest','him','himself','his',\n",
    "                                     'how','however','i','if','important','in','interest','interested','interesting',\n",
    "                                     'interests','into','is','it','its','itself','j','just','k','keep','keeps','kind',\n",
    "                                     'knew','know','known','knows','l','large','largely','last','later','latest',\n",
    "                                     'least','less','let','lets','like','likely','long','longer','longest','m',\n",
    "                                     'made','make','making','man','many','may','me','member','members','men',\n",
    "                                     'might','more','most','mostly','mr','mrs','much','must','my','myself',\n",
    "                                     'n','necessary','need','needed','needing','needs','never','new','new',\n",
    "                                     'newer','newest','next','no','nobody','non','noone','not','nothing',\n",
    "                                     'now','nowhere','number','numbers','o','of','off','often','old','older',\n",
    "                                     'oldest','on','once','one','only','open','opened','opening','opens','or',\n",
    "                                     'order','ordered','ordering','orders','other','others','our','out','over',\n",
    "                                     'p','part','parted','parting','parts','per','perhaps','place','places',\n",
    "                                     'point','pointed','pointing','points','possible','present','presented',\n",
    "                                     'presenting','presents','problem','problems','put','puts','q','quite','r',\n",
    "                                     'rather','really','right','right','room','rooms','s','said','same','saw',\n",
    "                                     'say','says','second','seconds','see','seem','seemed','seeming','seems',\n",
    "                                     'sees','several','shall','she','should','show','showed','showing','shows',\n",
    "                                     'side','sides','since','small','smaller','smallest','so','some','somebody',\n",
    "                                     'someone','something','somewhere','state','states','still','still','such',\n",
    "                                     'sure','t','take','taken','than','that','the','their','them','then','there',\n",
    "                                     'therefore','these','they','thing','things','think','thinks','this','those',\n",
    "                                     'though','thought','thoughts','three','through','thus','to','today','together',\n",
    "                                     'too','took','toward','turn','turned','turning','turns','two','u','under',\n",
    "                                     'until','up','upon','us','use','used','uses','v','very','w','want',\n",
    "                                     'wanted','wanting','wants','was','way','ways','we','well','wells','went','were',\n",
    "                                     'what','when','where','whether','which','while','who','whole','whose','why',\n",
    "                                     'will','with','within','without','work','worked','working','works','would','x',\n",
    "                                     'y','year','years','yet','you','young','younger','youngest','your','yours','z']\n",
    "def format(text):\n",
    "    words = [w.lower() for w in text.split() if len(w)>3]\n",
    "    wordswostop = [w for w in words if w not in customstopwords]\n",
    "    return wordswostop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do preparation of a corpora for genism LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [format(doc) for doc in groupedemails.Documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the series of experiments the best results were shown around 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"obama\" + 0.004*\"president\" + 0.004*\"party\" + 0.003*\"political\" + 0.003*\"house\" + 0.003*\"government\" + 0.002*\"clinton\" + 0.002*\"people\" + 0.002*\"support\" + 0.002*\"world\"'),\n",
       " (1,\n",
       "  '0.005*\"monday\" + 0.004*\"pakistan\" + 0.004*\"speech\" + 0.003*\"secretary\" + 0.003*\"send\" + 0.003*\"reid\" + 0.003*\"president\" + 0.003*\"message\" + 0.003*\"dinner\" + 0.003*\"spoke\"'),\n",
       " (2,\n",
       "  '0.006*\"secretary\" + 0.005*\"talk\" + 0.004*\"clinton\" + 0.004*\"speech\" + 0.004*\"haiti\" + 0.003*\"release\" + 0.003*\"people\" + 0.003*\"home\" + 0.003*\"send\" + 0.003*\"ashton\"'),\n",
       " (3,\n",
       "  '0.008*\"israel\" + 0.004*\"iran\" + 0.004*\"obama\" + 0.004*\"american\" + 0.004*\"foreign\" + 0.004*\"political\" + 0.004*\"peace\" + 0.004*\"print\" + 0.003*\"security\" + 0.003*\"president\"'),\n",
       " (4,\n",
       "  '0.007*\"secretary\" + 0.005*\"clinton\" + 0.005*\"press\" + 0.004*\"office\" + 0.003*\"speech\" + 0.003*\"huma\" + 0.003*\"president\" + 0.003*\"strategic\" + 0.003*\"people\" + 0.003*\"lona\"'),\n",
       " (5,\n",
       "  '0.005*\"libya\" + 0.004*\"united\" + 0.004*\"president\" + 0.004*\"department\" + 0.004*\"benghazi\" + 0.004*\"people\" + 0.003*\"agreement\" + 0.003*\"world\" + 0.003*\"obama\" + 0.003*\"house\"'),\n",
       " (6,\n",
       "  '0.008*\"obama\" + 0.006*\"president\" + 0.004*\"secretary\" + 0.004*\"house\" + 0.004*\"clinton\" + 0.004*\"people\" + 0.003*\"american\" + 0.003*\"security\" + 0.003*\"government\" + 0.003*\"party\"'),\n",
       " (7,\n",
       "  '0.005*\"president\" + 0.005*\"department\" + 0.004*\"house\" + 0.004*\"government\" + 0.003*\"people\" + 0.003*\"benghazi\" + 0.003*\"foreign\" + 0.003*\"information\" + 0.003*\"obama\" + 0.003*\"secretary\"'),\n",
       " (8,\n",
       "  '0.016*\"office\" + 0.016*\"secretary\" + 0.007*\"department\" + 0.007*\"house\" + 0.006*\"arrive\" + 0.006*\"route\" + 0.006*\"depart\" + 0.006*\"president\" + 0.005*\"obama\" + 0.005*\"private\"'),\n",
       " (9,\n",
       "  '0.004*\"people\" + 0.004*\"israel\" + 0.003*\"foreign\" + 0.003*\"government\" + 0.003*\"support\" + 0.003*\"united\" + 0.003*\"obama\" + 0.003*\"international\" + 0.003*\"department\" + 0.003*\"women\"'),\n",
       " (10,\n",
       "  '0.011*\"department\" + 0.010*\"benghazi\" + 0.010*\"house\" + 0.010*\"date\" + 0.009*\"information\" + 0.009*\"agreement\" + 0.009*\"sensitive\" + 0.009*\"select\" + 0.009*\"foia\" + 0.009*\"produced\"'),\n",
       " (11,\n",
       "  '0.008*\"report\" + 0.005*\"haiti\" + 0.005*\"women\" + 0.004*\"special\" + 0.004*\"security\" + 0.004*\"update\" + 0.004*\"secretary\" + 0.003*\"reuters\" + 0.003*\"thanks\" + 0.003*\"policy\"'),\n",
       " (12,\n",
       "  '0.008*\"women\" + 0.007*\"afghan\" + 0.005*\"mcchrystal\" + 0.005*\"clinton\" + 0.005*\"secretary\" + 0.004*\"afghanistan\" + 0.004*\"security\" + 0.004*\"hillary\" + 0.003*\"speech\" + 0.003*\"israel\"'),\n",
       " (13,\n",
       "  '0.009*\"china\" + 0.006*\"president\" + 0.005*\"obama\" + 0.005*\"policy\" + 0.004*\"israel\" + 0.004*\"deal\" + 0.004*\"northern\" + 0.004*\"minister\" + 0.003*\"netanyahu\" + 0.003*\"sent\"'),\n",
       " (14,\n",
       "  '0.004*\"draft\" + 0.004*\"speech\" + 0.004*\"email\" + 0.003*\"israel\" + 0.003*\"sent\" + 0.003*\"talk\" + 0.003*\"peace\" + 0.003*\"president\" + 0.003*\"office\" + 0.003*\"statement\"'),\n",
       " (15,\n",
       "  '0.006*\"secretary\" + 0.006*\"talk\" + 0.006*\"women\" + 0.006*\"office\" + 0.004*\"schedule\" + 0.003*\"message\" + 0.003*\"note\" + 0.003*\"home\" + 0.003*\"send\" + 0.003*\"president\"'),\n",
       " (16,\n",
       "  '0.005*\"diplomacy\" + 0.004*\"talk\" + 0.004*\"obama\" + 0.004*\"called\" + 0.003*\"public\" + 0.003*\"popover\" + 0.003*\"amazon\" + 0.003*\"secretary\" + 0.003*\"security\" + 0.003*\"sent\"'),\n",
       " (17,\n",
       "  '0.007*\"secretary\" + 0.007*\"percent\" + 0.006*\"office\" + 0.005*\"conference\" + 0.004*\"support\" + 0.004*\"israel\" + 0.003*\"agreement\" + 0.003*\"president\" + 0.003*\"press\" + 0.003*\"party\"'),\n",
       " (18,\n",
       "  '0.007*\"obama\" + 0.005*\"president\" + 0.003*\"talk\" + 0.003*\"american\" + 0.003*\"party\" + 0.003*\"week\" + 0.003*\"beck\" + 0.003*\"speech\" + 0.003*\"news\" + 0.003*\"skousen\"'),\n",
       " (19,\n",
       "  '0.009*\"talk\" + 0.006*\"update\" + 0.006*\"office\" + 0.005*\"please\" + 0.005*\"bill\" + 0.005*\"reuters\" + 0.005*\"schedule\" + 0.004*\"email\" + 0.004*\"boehner\" + 0.004*\"democrats\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 20\n",
    "lda = LdaMulticore(corpus, num_topics=N, workers=3, id2word=dictionary, alpha=0.02, eta=0.02, \n",
    "                   iterations=1000, random_state=42)\n",
    "lda.print_topics(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks fine but there are some duplicates  0,6,18,19 and 4 represents the same topic related to elections.\n",
    "5,7,10 topics around Libya and 3,9,14,17 conversations related to Israel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"obama\" + 0.004*\"president\" + 0.003*\"party\" + 0.003*\"political\" + 0.003*\"house\" + 0.003*\"government\" + 0.003*\"secretary\" + 0.002*\"people\" + 0.002*\"called\" + 0.002*\"office\"'),\n",
       " (1,\n",
       "  '0.004*\"monday\" + 0.004*\"secretary\" + 0.003*\"speech\" + 0.003*\"nuclear\" + 0.003*\"clinton\" + 0.003*\"issues\" + 0.003*\"washington\" + 0.003*\"message\" + 0.003*\"print\" + 0.003*\"reid\"'),\n",
       " (2,\n",
       "  '0.005*\"send\" + 0.004*\"secretary\" + 0.004*\"haiti\" + 0.003*\"release\" + 0.003*\"clinton\" + 0.003*\"schedule\" + 0.003*\"statement\" + 0.003*\"message\" + 0.003*\"cheryl\" + 0.003*\"talk\"'),\n",
       " (3,\n",
       "  '0.007*\"israel\" + 0.004*\"print\" + 0.004*\"american\" + 0.004*\"political\" + 0.004*\"obama\" + 0.003*\"foreign\" + 0.003*\"president\" + 0.003*\"peace\" + 0.003*\"speech\" + 0.003*\"iran\"'),\n",
       " (4,\n",
       "  '0.007*\"secretary\" + 0.004*\"clinton\" + 0.004*\"office\" + 0.004*\"press\" + 0.004*\"speech\" + 0.004*\"president\" + 0.003*\"lona\" + 0.003*\"people\" + 0.003*\"assistant\" + 0.003*\"cheryl\"'),\n",
       " (5,\n",
       "  '0.004*\"libya\" + 0.004*\"united\" + 0.003*\"office\" + 0.003*\"department\" + 0.003*\"president\" + 0.003*\"people\" + 0.003*\"hillary\" + 0.003*\"obama\" + 0.003*\"agreement\" + 0.003*\"world\"'),\n",
       " (6,\n",
       "  '0.008*\"obama\" + 0.006*\"president\" + 0.004*\"secretary\" + 0.004*\"house\" + 0.004*\"clinton\" + 0.004*\"people\" + 0.003*\"american\" + 0.003*\"security\" + 0.003*\"government\" + 0.003*\"party\"'),\n",
       " (7,\n",
       "  '0.005*\"president\" + 0.005*\"department\" + 0.004*\"house\" + 0.004*\"government\" + 0.003*\"obama\" + 0.003*\"foreign\" + 0.003*\"people\" + 0.003*\"security\" + 0.003*\"clinton\" + 0.003*\"secretary\"'),\n",
       " (8,\n",
       "  '0.015*\"office\" + 0.014*\"secretary\" + 0.006*\"department\" + 0.006*\"house\" + 0.006*\"arrive\" + 0.006*\"president\" + 0.006*\"obama\" + 0.005*\"route\" + 0.005*\"depart\" + 0.005*\"private\"'),\n",
       " (9,\n",
       "  '0.004*\"women\" + 0.004*\"israel\" + 0.004*\"people\" + 0.003*\"party\" + 0.003*\"support\" + 0.003*\"obama\" + 0.003*\"government\" + 0.003*\"foreign\" + 0.003*\"vote\" + 0.003*\"american\"'),\n",
       " (10,\n",
       "  '0.008*\"department\" + 0.007*\"house\" + 0.007*\"benghazi\" + 0.007*\"information\" + 0.006*\"agreement\" + 0.006*\"date\" + 0.006*\"produced\" + 0.006*\"select\" + 0.006*\"dept\" + 0.006*\"foia\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 11\n",
    "lda = LdaMulticore(corpus, num_topics=N, workers=3, id2word=dictionary, alpha=0.02, eta=0.02, \n",
    "                   iterations=1000, random_state=42)\n",
    "lda.print_topics(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not look better after reduction as lost topic about China..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of modeled topics 2 and perplexity for the corpora -9.271516150431834\n",
      "Number of modeled topics 3 and perplexity for the corpora -9.404023712125204\n",
      "Number of modeled topics 4 and perplexity for the corpora -9.536958166659696\n",
      "Number of modeled topics 5 and perplexity for the corpora -9.646190592954412\n",
      "Number of modeled topics 6 and perplexity for the corpora -9.738715685847344\n",
      "Number of modeled topics 7 and perplexity for the corpora -9.694275673091616\n",
      "Number of modeled topics 8 and perplexity for the corpora -9.76478951162569\n",
      "Number of modeled topics 9 and perplexity for the corpora -9.803833055632605\n",
      "Number of modeled topics 10 and perplexity for the corpora -9.871231882430763\n",
      "Number of modeled topics 11 and perplexity for the corpora -9.924778216326255\n",
      "Number of modeled topics 12 and perplexity for the corpora -9.946776065371559\n",
      "Number of modeled topics 13 and perplexity for the corpora -10.002614743762406\n",
      "Number of modeled topics 14 and perplexity for the corpora -10.03228220774034\n",
      "Number of modeled topics 15 and perplexity for the corpora -10.076516671338668\n",
      "Number of modeled topics 16 and perplexity for the corpora -10.106648125854528\n",
      "Number of modeled topics 17 and perplexity for the corpora -10.138443410651574\n",
      "Number of modeled topics 18 and perplexity for the corpora -10.180649116298566\n",
      "Number of modeled topics 19 and perplexity for the corpora -10.209185186260505\n",
      "Number of modeled topics 20 and perplexity for the corpora -10.234330989364327\n",
      "Number of modeled topics 21 and perplexity for the corpora -10.258833868744475\n",
      "Number of modeled topics 22 and perplexity for the corpora -10.275824516287168\n",
      "Number of modeled topics 23 and perplexity for the corpora -10.321125889005843\n",
      "Number of modeled topics 24 and perplexity for the corpora -10.369423860638666\n",
      "Number of modeled topics 25 and perplexity for the corpora -10.391432789738888\n",
      "Number of modeled topics 26 and perplexity for the corpora -10.422387848702975\n",
      "Number of modeled topics 27 and perplexity for the corpora -10.4472182494173\n",
      "Number of modeled topics 28 and perplexity for the corpora -10.467528918403282\n",
      "Number of modeled topics 29 and perplexity for the corpora -10.480392528299328\n",
      "Number of modeled topics 30 and perplexity for the corpora -10.520225726975482\n",
      "Number of modeled topics 31 and perplexity for the corpora -10.53264024483846\n",
      "Number of modeled topics 32 and perplexity for the corpora -10.557412872594355\n",
      "Number of modeled topics 33 and perplexity for the corpora -10.576651374422292\n",
      "Number of modeled topics 34 and perplexity for the corpora -10.58156266660347\n",
      "Number of modeled topics 35 and perplexity for the corpora -10.59580651166505\n",
      "Number of modeled topics 36 and perplexity for the corpora -10.613892692688736\n",
      "Number of modeled topics 37 and perplexity for the corpora -10.646881370470034\n",
      "Number of modeled topics 38 and perplexity for the corpora -10.697660611297819\n",
      "Number of modeled topics 39 and perplexity for the corpora -10.73351672740058\n",
      "Number of modeled topics 40 and perplexity for the corpora -10.74753314259966\n",
      "Number of modeled topics 41 and perplexity for the corpora -10.774151858449649\n",
      "Number of modeled topics 42 and perplexity for the corpora -10.800757523304316\n",
      "Number of modeled topics 43 and perplexity for the corpora -10.826295728670656\n",
      "Number of modeled topics 44 and perplexity for the corpora -10.86608077008227\n",
      "Number of modeled topics 45 and perplexity for the corpora -10.867893402603379\n",
      "Number of modeled topics 46 and perplexity for the corpora -10.878842394110974\n",
      "Number of modeled topics 47 and perplexity for the corpora -10.918070037819923\n",
      "Number of modeled topics 48 and perplexity for the corpora -10.921561948719646\n",
      "Number of modeled topics 49 and perplexity for the corpora -10.95063730116487\n",
      "Number of modeled topics 50 and perplexity for the corpora -10.975921272265593\n"
     ]
    }
   ],
   "source": [
    "for N in range(2,51):\n",
    "    lda = LdaMulticore(corpus, num_topics=N, workers=3, id2word=dictionary, alpha=0.02, eta=0.02, \n",
    "                   iterations=1000, random_state=42)\n",
    "    print('Number of modeled topics {} and perplexity for the corpora {}'.format(N, lda.log_perplexity(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we don't have a specific heldout cropora to evaluate our results properly. From the observed results we can see the that potentialy 6-7 topics can be a good fit as of a slight decrese of 7 between scores for 6 and 8 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
